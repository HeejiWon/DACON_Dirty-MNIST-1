{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   \n",
    "# Setting\n",
    "> * 구글드라이브-코랩 연동\n",
    "* 사용할 기본 모듈들 import\n",
    "* 경로설정\n",
    "* 시드고정함수\n",
    "##   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글드라이브와 연동\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# modules\n",
    "import tensorflow as tf\n",
    "from tensorflow import  keras\n",
    "!pip install tensorflow_addons\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import os\n",
    "\n",
    "# 데이터 경로 설정\n",
    "os.chdir('/content/drive/MyDrive/project_dataset/dacon_v2')\n",
    "train_dir = \"/content/drive/MyDrive/project_dataset/dacon_v2/dirty_mnist_2nd\"\n",
    "test_dir = \"/content/drive/MyDrive/project_dataset/dacon_v2/test_route\"\n",
    "\n",
    "# GPU 확인\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "# seed 고정 함수\n",
    "def seed_everything(seed = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "SEED = 1234 # global seed    \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "# Data Preparing\n",
    "> * 256*256 train image 5만장 45000 : 4000 : 1000 분할하기\n",
    "* keras의 ImageDataGenerator를 이용한 augmentation\n",
    "* pretrained 모델을 사용하기 위해 grayscaled 데이터를 rgb로 변환\n",
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_size = 49000\n",
    "train_size = 45000\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# df format for making generator\n",
    "meta_df = pd.read_csv('dirty_mnist_2nd_answer.csv')\n",
    "meta_df['index'] = meta_df['index'].apply(lambda x: str(\"{0:05d}\".format(x))+'.png')\n",
    "columns = list(meta_df.columns[1:])\n",
    "\n",
    "\n",
    "# train + valid : test = 49000 : 1000\n",
    "train_full_df = meta_df.iloc[:full_train_size,:]\n",
    "holdout_df = meta_df.iloc[full_train_size:,:]\n",
    "\n",
    "\n",
    "# Augmentation strategy\n",
    "train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                             rotation_range = 10,\n",
    "                             width_shift_range = 0.1,\n",
    "                             height_shift_range = 0.1,\n",
    "                             horizontal_flip = True,\n",
    "                             vertical_flip = True,\n",
    "                             validation_split = 1 - train_size / full_train_size)\n",
    "\n",
    "holdout_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                             rotation_range = 10,\n",
    "                             width_shift_range = 0.1,\n",
    "                             height_shift_range = 0.1,\n",
    "                             horizontal_flip = True,\n",
    "                             vertical_flip = True)\n",
    "\n",
    "# generator\n",
    "train_gen = train_datagen.flow_from_dataframe(dataframe = train_full_df,        \n",
    "                                        directory = train_dir,       \n",
    "                                        x_col = 'index',               \n",
    "                                        y_col = columns,                \n",
    "                                        batch_size = batch_size,               \n",
    "                                        seed = SEED,\n",
    "                                        color_mode = \"rgb\",           \n",
    "                                        class_mode = 'raw',\n",
    "                                        target_size = (256, 256),       \n",
    "                                        subset = 'training')\n",
    "\n",
    "valid_gen = train_datagen.flow_from_dataframe(dataframe = train_full_df,        \n",
    "                                        directory = train_dir,       \n",
    "                                        x_col = 'index',               \n",
    "                                        y_col = columns,                \n",
    "                                        batch_size = batch_size,               \n",
    "                                        seed = SEED,\n",
    "                                        color_mode = \"rgb\",           \n",
    "                                        class_mode = 'raw',\n",
    "                                        target_size = (256, 256),       \n",
    "                                        subset = 'validation')\n",
    "\n",
    "holdout_gen = holdout_datagen.flow_from_dataframe(dataframe = holdout_df,        \n",
    "                                        directory = train_dir,       \n",
    "                                        x_col = 'index',               \n",
    "                                        y_col = columns,                \n",
    "                                        batch_size = batch_size,               \n",
    "                                        seed = SEED,\n",
    "                                        color_mode = \"rgb\",           \n",
    "                                        class_mode = 'raw',\n",
    "                                        target_size = (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "# Model\n",
    "> * seed 고정은 Cell마다 해줘야된다..\n",
    "* keras.application의 pretrained 모델을 하나의 layer처럼 가져와 사용\n",
    "* top layers는 따로 선언해준다. output 층은 레이블 수 만큼의 뉴런을 갖도록 한다. \n",
    "* RAdam 옵티마이저를 사용하며 warm restart를 하는 cosine decay 방식의 lr schedule를 적용한다.\n",
    "* Lookahead 기법을 적용하여 수렴 품질을 개선한다.\n",
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "model_save_path = \"./model/effi_b5_0227.h5\"\n",
    "\n",
    "# base model\n",
    "base_model = tf.keras.applications.EfficientNetB5(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_shape=(256, 256, 3)\n",
    ")\n",
    "# architecture\n",
    "effi_b5 = keras.Sequential([\n",
    "                            base_model,\n",
    "                            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                            tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "                            tf.keras.layers.Dense(1024, kernel_initializer='he_normal'),\n",
    "                            tf.keras.layers.BatchNormalization(),\n",
    "                            tf.keras.layers.LeakyReLU(0.1),\n",
    "                            tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "                            tf.keras.layers.Dense(512, kernel_initializer='he_normal'),\n",
    "                            tf.keras.layers.BatchNormalization(),\n",
    "                            tf.keras.layers.LeakyReLU(0.1),\n",
    "                            tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "                            tf.keras.layers.Dense(26, kernel_initializer='glorot_normal', activation='sigmoid'),\n",
    "]) \n",
    "\n",
    "# metrics\n",
    "BAcc = keras.metrics.BinaryAccuracy(name='binary_accuracy')\n",
    "\n",
    "# LR schedule - Cosine Annealing\n",
    "n_epochs = 100\n",
    "first_decay_steps = ((45000 // batch_size) * n_epochs) // 5\n",
    "initial_learning_rate = 0.003\n",
    "\n",
    "lr_decayed_fn = (\n",
    "  tf.keras.experimental.CosineDecayRestarts(\n",
    "      initial_learning_rate,\n",
    "      first_decay_steps,\n",
    "      t_mul=2.0,\n",
    "      m_mul=0.95))\n",
    "\n",
    "# optimizer\n",
    "radam = tfa.optimizers.RectifiedAdam(learning_rate = lr_decayed_fn,\n",
    "                                     weight_decay = 0.0001,\n",
    "                                     warmup_proportion = 0,\n",
    "                                     min_lr = 1e-6)\n",
    "# lookahead\n",
    "ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
    "\n",
    "# compile & callbacks\n",
    "effi_b5.compile(optimizer = ranger, loss = \"binary_crossentropy\", metrics = [BAcc])\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(model_save_path, \n",
    "                                             save_best_only=True, verbose=1)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience = 20, restore_best_weights = True)\n",
    "\n",
    "\n",
    "# fitting\n",
    "history = effi_b5.fit(train_gen, epochs = n_epochs, \n",
    "                      validation_data = valid_gen, \n",
    "                      callbacks = [checkpoint, early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize = (16, 9))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "# (pseudo) Transfer Learning\n",
    "> * 위와 같이 학습한 모델에서 clf head는 버리고 몸통부분만 가져온다.\n",
    "* 새로운 clf head를 정의하고 그대로 복사한 몸통부분 가중치는 freezing한다.\n",
    "* clf head만 우선적으로 몇 에폭 학습시킨 후, 몸통부분의 동결을 해제하고 전체 모델에 대해 update 진행\n",
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 - train only head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "# pretrain model load\n",
    "pretrain = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "# Body weights freezing\n",
    "pretrain.layers[0].trainable = False\n",
    "\n",
    "# archtecture\n",
    "effi_transfer = keras.Sequential([\n",
    "                                  pretrained.layers[0],\n",
    "                                  tf.keras.layers.GlobalAveragePooling2D(), \n",
    "                                  tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "                                  tf.keras.layers.Dense(1024, kernel_initializer='he_normal'),\n",
    "                                  tf.keras.layers.BatchNormalization(),\n",
    "                                  tf.keras.layers.LeakyReLU(0.2),\n",
    "                                  tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "                                  tf.keras.layers.Dense(26, kernel_initializer='glorot_normal', activation='sigmoid'),\n",
    "])\n",
    "\n",
    "\n",
    "# compile\n",
    "effi_transfer.compile(optimizer = 'adam', loss = \"binary_crossentropy\", metrics = [BAcc])\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('./model/effi_head_tuning.h5', \n",
    "                                             save_best_only=True, verbose=1)\n",
    "\n",
    "# fitting\n",
    "effi_transfer.fit(train_gen, epochs = 5, \n",
    "                   validation_data = valid_gen, \n",
    "                   callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 - fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "# melting\n",
    "effi_transfer.layers[0].trainable = True\n",
    "\n",
    "# compile - 가중치 동결 전 후 필히 compile을 다시 해줘야 함.\n",
    "effi_transfer.compile(optimizer = ranger, loss = \"binary_crossentropy\", metrics = [BAcc])\n",
    "\n",
    "# callbacks\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('./model/effi_head_change.h5', \n",
    "                                             save_best_only=True, verbose=1,\n",
    "                                             mode = \"min\",monitor = \"val_loss\")\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience = 20, restore_best_weights = True)\n",
    "\n",
    "# fitting\n",
    "effi_transfer.fit(train_gen, epochs = n_epochs, \n",
    "                   validation_data = valid_gen, \n",
    "                   callbacks = [checkpoint, early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   \n",
    "# Prediction\n",
    "> * Test Time Augmentation을 통해 데이터의 변동에 대한 robustness 향상시키기\n",
    "* Monte Carlo Dropout을 통해 드랍아웃 모델의 앙상블 실현\n",
    "* for loop 두 개로 위의 두 가지 기법을 동시에 적용한다. (TTA 루프 안에서 MC dropout 루핑)\n",
    "##   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "test_submit = pd.read_csv('sample_submission.csv')\n",
    "test_df = test_submit.copy()\n",
    "test_df['index'] = test_submit['index'].apply(lambda x: str(\"{:0>5d}\".format(x))+'.png')\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "# TTA를 위해 train set과 동일한 augmentation을 적용한다.\n",
    "test_gen = ImageDataGenerator(rescale=1./255.,\n",
    "                              rotation_range = 10,\n",
    "                              width_shift_range = 0.1,\n",
    "                              height_shift_range = 0.1,\n",
    "                              horizontal_flip = True,\n",
    "                              vertical_flip = True,\n",
    "                              fill_mode = \"nearest\")\n",
    "\n",
    "# 제출을 위한 예측 데이터 프레임을 만들 때 순서가 유지되어야하므로\n",
    "# shuffle을 False로 설정해줘야한다.\n",
    "test_gen = test_gen.flow_from_dataframe(dataframe = test_df,        \n",
    "                                        directory='./test_route/test_dirty_mnist_2nd',      \n",
    "                                        x_col='index',                             \n",
    "                                        batch_size = batch_size,               \n",
    "                                        shuffle = False,                \n",
    "                                        color_mode = \"rgb\",           \n",
    "                                        class_mode=None,\n",
    "                                        target_size=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "# load_model\n",
    "model = keras.models.load_model(\"./model/effi_0301.h5\")\n",
    "\n",
    "# MC DO \n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)   # forcing training mode\n",
    "\n",
    "# replace normal DO layers -> MC DO layers\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCDropout(layer.rate) if isinstance(layer, keras.layers.Dropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "\n",
    "# compile\n",
    "mc_model.compile(optimizer = ranger, loss = \"binary_crossentropy\", metrics = [BAcc])\n",
    "mc_model.set_weights(model.get_weights())\n",
    "\n",
    "\n",
    "# Setting for TTA and MCDO\n",
    "tta_steps = 50\n",
    "mc_steps = 50\n",
    "predictions = [] \n",
    "test_size = len(test_df)\n",
    "columns = list(test_df.columns[1:])\n",
    "filename = \"efficientB5.csv\"\n",
    "\n",
    "\n",
    "# double loop \n",
    "for i in range(tta_steps):\n",
    "    print(\"*********** augment iter {} ***********\".format(i))\n",
    "    # 모델의 각 예측은 랜덤하게 드롭아웃된 서로다른 신경망들로부터 나옴\n",
    "    mc_pred = [mc_model.predict_generator(generator = test_gen, \n",
    "                                          steps = test_size // batch_size,\n",
    "                                          verbose = 1) for sample in range(mc_steps)]\n",
    "    # 드롭아웃 앙상블\n",
    "    mc_pred_means = np.mean(mc_pred, axis = 0)\n",
    "    # 각 augmentation별로 예측 저장\n",
    "    predictions.append(mc_pred_means)\n",
    "\n",
    "\n",
    "# final prediction\n",
    "pred = np.mean(predictions, axis=0) # augmentation에 대한 평균\n",
    "pred_sub = pred.copy()\n",
    "\n",
    "# get label\n",
    "pred_sub = pred_sub.round()\n",
    "\n",
    "# create DF\n",
    "res = pd.DataFrame(pred_sub, columns = columns )\n",
    "int_rest = res.astype(int)\n",
    "submit = pd.concat([test_submit.iloc[:,0],int_rest], axis = 1)\n",
    "submit.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습률 확인하기 \n",
    "import keras.backend as K\n",
    "\n",
    "print(K.eval(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training_labels.csv file and creating the instances\n",
    "train_data = pd.read_csv('training_labels.csv')\n",
    "Y = train_data[['label']]\n",
    "\n",
    "kf = KFold(n_splits = 5)                       \n",
    "\n",
    "# Create an instance of the ImageDataGenerator class\n",
    "idg = ImageDataGenerator(width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=0.3,\n",
    "                         fill_mode='nearest',\n",
    "                         horizontal_flip = True,\n",
    "                         rescale=1./255)\n",
    "\n",
    "# Auxiliary function for getting model name in each of the k iterations\n",
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'\n",
    "\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDAITON_LOSS = []\n",
    "\n",
    "save_dir = '/saved_models/'\n",
    "fold_var = 1\n",
    "\n",
    "for train_index, val_index in kf.split(np.zeros(n_samples),Y):\n",
    "    training_data = train_data.iloc[train_index]\n",
    "    validation_data = train_data.iloc[val_index]\n",
    "\n",
    "    train_data_generator = idg.flow_from_dataframe(training_data, directory = image_dir,\n",
    "                               x_col = \"filename\", y_col = \"label\",\n",
    "                               class_mode = \"categorical\", shuffle = True)\n",
    "    valid_data_generator  = idg.flow_from_dataframe(validation_data, directory = image_dir,\n",
    "                            x_col = \"filename\", y_col = \"label\",\n",
    "                            class_mode = \"categorical\", shuffle = True)\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = create_new_model()\n",
    "    # COMPILE NEW MODEL\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # CREATE CALLBACKS\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var), \n",
    "                            monitor='val_accuracy', verbose=1, \n",
    "                            save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    history = model.fit(train_data_generator,\n",
    "                epochs=num_epochs,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_data=valid_data_generator)\n",
    "\n",
    "    # LOAD BEST MODEL to evaluate the performance of the model\n",
    "    model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n",
    "    \n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    fold_var += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
