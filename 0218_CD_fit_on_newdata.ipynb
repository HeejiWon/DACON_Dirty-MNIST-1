{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0218_fit_on_newdata",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNS60nA1XVJ6AST7V5yhZX4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changdaeoh/DACON_Dirty-MNIST/blob/main/0218_fit_on_newdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg9mwpGqNnfy",
        "outputId": "723ff117-5b76-493a-867d-68b4d3ceda22"
      },
      "source": [
        "# 구글드라이브와 연동\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5zvs587OXCK",
        "outputId": "e3f0e678-0719-4a56-f087-bda084a8ace0"
      },
      "source": [
        "# setting\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import  keras\r\n",
        "\r\n",
        "!pip install tensorflow-addons\r\n",
        "import tensorflow_addons as tfa\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "import os\r\n",
        "os.chdir('/content/drive/MyDrive/project_dataset/dacon_v2')\r\n",
        "\r\n",
        "\r\n",
        "# 데이터 경로\r\n",
        "train_dir = \"/content/drive/MyDrive/project_dataset/dacon_v2/dirty_mnist_2nd\"\r\n",
        "test_dir = \"/content/drive/MyDrive/project_dataset/dacon_v2/test_route\"\r\n",
        "\r\n",
        "# GPU 확인\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "    raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlIyjY634ixA"
      },
      "source": [
        "# # 현재 경로에 새로운 디렉토리 생성\r\n",
        "# # 생성한 디렉토리에 zip파일 압축해제\r\n",
        "\r\n",
        "from google.colab import output\r\n",
        "\r\n",
        "!mkdir \"dirty_mnist_2nd\"\r\n",
        "!unzip \"dirty_mnist_2nd.zip\" -d \"dirty_mnist_2nd\"\r\n",
        "\r\n",
        "# !mkdir \"test_dirty_mnist\"\r\n",
        "# !unzip \"test_dirty_mnist.zip\" -d \"./test_dirty_mnist\"\r\n",
        "\r\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADHj9SeL8wqk"
      },
      "source": [
        "# Training \r\n",
        "### 1. Data Preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGK023t2-C8_",
        "outputId": "26d72cad-756e-4554-ecbd-46d3551a638b"
      },
      "source": [
        "meta_df = pd.read_csv('dirty_mnist_2nd_answer.csv')\r\n",
        "meta_df['index'] = meta_df['index'].apply(lambda x: str(\"{0:05d}\".format(x))+'.png')\r\n",
        "columns = list(meta_df.columns[1:])\r\n",
        "\r\n",
        "\r\n",
        "# Data Augumentation\r\n",
        "datagen_ag = ImageDataGenerator(rescale=1./255., \r\n",
        "                               rotation_range = 10,\r\n",
        "                               width_shift_range = 0.2,\r\n",
        "                               height_shift_range = 0.2,\r\n",
        "                               horizontal_flip = True,\r\n",
        "                               vertical_flip = True,\r\n",
        "                               validation_split=0.1,\r\n",
        "                               fill_mode = \"nearest\")\r\n",
        "\r\n",
        "# generator_ag\r\n",
        "train_gen_ag = datagen_ag.flow_from_dataframe(dataframe = meta_df,        \r\n",
        "                                        directory = train_dir,       \r\n",
        "                                        x_col='index',               \r\n",
        "                                        y_col=columns,                \r\n",
        "                                        batch_size = 32,               \r\n",
        "                                        seed = 42,\r\n",
        "                                        color_mode = \"rgb\",           \r\n",
        "                                        class_mode='raw',\r\n",
        "                                        target_size=(256, 256),       \r\n",
        "                                        subset='training')\r\n",
        "val_gen_ag = datagen_ag.flow_from_dataframe(dataframe = meta_df,\r\n",
        "                                        directory = train_dir,\r\n",
        "                                        x_col='index',\r\n",
        "                                        y_col=columns,\r\n",
        "                                        batch_size = 32,\r\n",
        "                                        seed = 42,\r\n",
        "                                        color_mode = \"rgb\",\r\n",
        "                                        class_mode='raw',\r\n",
        "                                        target_size=(256, 256),\r\n",
        "                                        subset='validation')\r\n",
        "\r\n",
        "\r\n",
        "# datagen_1 = ImageDataGenerator(rescale=1./255., validation_split=0.1)\r\n",
        "\r\n",
        "# # not ag\r\n",
        "# train_gen = datagen_1.flow_from_dataframe(dataframe = meta_df,        \r\n",
        "#                                         directory = train_dir,       \r\n",
        "#                                         x_col='index',               \r\n",
        "#                                         y_col=columns,                \r\n",
        "#                                         batch_size = 32,               \r\n",
        "#                                         seed = 42,\r\n",
        "#                                         color_mode = \"rgb\",           \r\n",
        "#                                         class_mode='raw',\r\n",
        "#                                         target_size=(256, 256),       \r\n",
        "#                                         subset='training')\r\n",
        "# val_gen = datagen_1.flow_from_dataframe(dataframe = meta_df,\r\n",
        "#                                         directory = train_dir,\r\n",
        "#                                         x_col='index',\r\n",
        "#                                         y_col=columns,\r\n",
        "#                                         batch_size = 32,\r\n",
        "#                                         seed = 42,\r\n",
        "#                                         color_mode = \"rgb\",\r\n",
        "#                                         class_mode='raw',\r\n",
        "#                                         target_size=(256, 256),\r\n",
        "#                                         subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 45000 validated image filenames.\n",
            "Found 5000 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFvPPAss-uPt"
      },
      "source": [
        "### 2. Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF3U6HTA-l2t",
        "outputId": "e286c8f2-345c-4794-ad9b-83e4967d2aff"
      },
      "source": [
        "base_model = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False,\r\n",
        "                                                                      weights='imagenet',\r\n",
        "                                                                      input_shape = (256, 256, 3))\r\n",
        "# model body\r\n",
        "incep_res = keras.Sequential([\r\n",
        "                              base_model,\r\n",
        "\r\n",
        "                              keras.layers.GlobalAveragePooling2D(), \r\n",
        "                              keras.layers.Dense(1024, kernel_initializer='he_normal'),\r\n",
        "                              keras.layers.BatchNormalization(),\r\n",
        "                              keras.layers.Activation('elu'),\r\n",
        "                              keras.layers.Dense(512, kernel_initializer='he_normal'),\r\n",
        "                              keras.layers.BatchNormalization(),\r\n",
        "                              keras.layers.Activation('elu'),\r\n",
        "                              keras.layers.Dense(256, kernel_initializer='he_normal'),\r\n",
        "                              keras.layers.BatchNormalization(),\r\n",
        "                              keras.layers.Activation('elu'),\r\n",
        "                              keras.layers.Dense(26, kernel_initializer='glorot_normal', activation='sigmoid')\r\n",
        "])      \r\n",
        "\r\n",
        "# optimizer & metrics\r\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,\r\n",
        "#                                                           decay_steps=10000,\r\n",
        "#                                                           decay_rate=0.9)\r\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\r\n",
        "BAcc = keras.metrics.BinaryAccuracy()\r\n",
        "\r\n",
        "# model compile\r\n",
        "incep_res.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [BAcc])\r\n",
        "\r\n",
        "# callbacks\r\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(f'./model/incep_res_cd_0219_v1.h5', \r\n",
        "                                             save_best_only=True, verbose=1)\r\n",
        "early_stop_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\r\n",
        "\r\n",
        "# fitting\r\n",
        "history = incep_res.fit(train_gen_ag, epochs = 35, \r\n",
        "                        validation_data = val_gen_ag, \r\n",
        "                        callbacks = [checkpoint, early_stop_cb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 5s 0us/step\n",
            "Epoch 1/35\n",
            "1407/1407 [==============================] - 895s 605ms/step - loss: 0.6916 - binary_accuracy: 0.5485 - val_loss: 0.6706 - val_binary_accuracy: 0.5936\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.67056, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 2/35\n",
            "1407/1407 [==============================] - 850s 604ms/step - loss: 0.6416 - binary_accuracy: 0.6228 - val_loss: 0.6288 - val_binary_accuracy: 0.6420\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.67056 to 0.62884, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 3/35\n",
            "1407/1407 [==============================] - 852s 605ms/step - loss: 0.6100 - binary_accuracy: 0.6574 - val_loss: 0.6182 - val_binary_accuracy: 0.6619\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.62884 to 0.61823, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 4/35\n",
            "1407/1407 [==============================] - 854s 606ms/step - loss: 0.5875 - binary_accuracy: 0.6795 - val_loss: 0.5993 - val_binary_accuracy: 0.6800\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.61823 to 0.59933, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 5/35\n",
            "1407/1407 [==============================] - 852s 605ms/step - loss: 0.5672 - binary_accuracy: 0.6988 - val_loss: 0.5749 - val_binary_accuracy: 0.7012\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.59933 to 0.57490, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 6/35\n",
            "1407/1407 [==============================] - 868s 616ms/step - loss: 0.5475 - binary_accuracy: 0.7150 - val_loss: 0.5430 - val_binary_accuracy: 0.7214\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.57490 to 0.54299, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 7/35\n",
            "1407/1407 [==============================] - 856s 608ms/step - loss: 0.5279 - binary_accuracy: 0.7317 - val_loss: 0.5279 - val_binary_accuracy: 0.7328\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.54299 to 0.52788, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 8/35\n",
            "1407/1407 [==============================] - 840s 597ms/step - loss: 0.5129 - binary_accuracy: 0.7436 - val_loss: 0.4997 - val_binary_accuracy: 0.7550\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.52788 to 0.49974, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 9/35\n",
            "1407/1407 [==============================] - 850s 604ms/step - loss: 0.4869 - binary_accuracy: 0.7650 - val_loss: 0.4833 - val_binary_accuracy: 0.7682\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.49974 to 0.48333, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 10/35\n",
            "1407/1407 [==============================] - 833s 592ms/step - loss: 0.4689 - binary_accuracy: 0.7775 - val_loss: 0.4907 - val_binary_accuracy: 0.7727\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.48333\n",
            "Epoch 11/35\n",
            "1407/1407 [==============================] - 829s 589ms/step - loss: 0.4560 - binary_accuracy: 0.7883 - val_loss: 0.4409 - val_binary_accuracy: 0.7964\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.48333 to 0.44092, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 12/35\n",
            "1407/1407 [==============================] - 845s 600ms/step - loss: 0.4347 - binary_accuracy: 0.8007 - val_loss: 0.4324 - val_binary_accuracy: 0.8027\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.44092 to 0.43243, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 13/35\n",
            "1407/1407 [==============================] - 861s 612ms/step - loss: 0.4178 - binary_accuracy: 0.8121 - val_loss: 0.4186 - val_binary_accuracy: 0.8125\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.43243 to 0.41857, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 14/35\n",
            "1407/1407 [==============================] - 859s 610ms/step - loss: 0.4023 - binary_accuracy: 0.8217 - val_loss: 0.4137 - val_binary_accuracy: 0.8136\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.41857 to 0.41372, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 15/35\n",
            "1407/1407 [==============================] - 857s 609ms/step - loss: 0.3917 - binary_accuracy: 0.8282 - val_loss: 0.3976 - val_binary_accuracy: 0.8222\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.41372 to 0.39756, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 16/35\n",
            "1407/1407 [==============================] - 861s 612ms/step - loss: 0.3828 - binary_accuracy: 0.8327 - val_loss: 0.3890 - val_binary_accuracy: 0.8285\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.39756 to 0.38904, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 17/35\n",
            "1407/1407 [==============================] - 853s 606ms/step - loss: 0.3742 - binary_accuracy: 0.8374 - val_loss: 0.3911 - val_binary_accuracy: 0.8290\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.38904\n",
            "Epoch 18/35\n",
            "1407/1407 [==============================] - 841s 598ms/step - loss: 0.3667 - binary_accuracy: 0.8415 - val_loss: 0.3831 - val_binary_accuracy: 0.8311\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.38904 to 0.38307, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 19/35\n",
            "1407/1407 [==============================] - 843s 599ms/step - loss: 0.3614 - binary_accuracy: 0.8445 - val_loss: 0.3789 - val_binary_accuracy: 0.8338\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.38307 to 0.37887, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 20/35\n",
            "1407/1407 [==============================] - 848s 602ms/step - loss: 0.3532 - binary_accuracy: 0.8482 - val_loss: 0.3700 - val_binary_accuracy: 0.8392\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.37887 to 0.37002, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 21/35\n",
            "1407/1407 [==============================] - 843s 599ms/step - loss: 0.3497 - binary_accuracy: 0.8501 - val_loss: 0.3686 - val_binary_accuracy: 0.8395\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.37002 to 0.36861, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 22/35\n",
            "1407/1407 [==============================] - 839s 596ms/step - loss: 0.3417 - binary_accuracy: 0.8545 - val_loss: 0.3653 - val_binary_accuracy: 0.8415\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.36861 to 0.36528, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 23/35\n",
            "1407/1407 [==============================] - 853s 606ms/step - loss: 0.3391 - binary_accuracy: 0.8552 - val_loss: 0.3762 - val_binary_accuracy: 0.8367\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.36528\n",
            "Epoch 24/35\n",
            "1407/1407 [==============================] - 855s 608ms/step - loss: 0.3336 - binary_accuracy: 0.8584 - val_loss: 0.3542 - val_binary_accuracy: 0.8475\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.36528 to 0.35423, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 25/35\n",
            "1407/1407 [==============================] - 854s 607ms/step - loss: 0.3295 - binary_accuracy: 0.8603 - val_loss: 0.3576 - val_binary_accuracy: 0.8461\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.35423\n",
            "Epoch 26/35\n",
            "1407/1407 [==============================] - 858s 609ms/step - loss: 0.3242 - binary_accuracy: 0.8632 - val_loss: 0.3504 - val_binary_accuracy: 0.8495\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.35423 to 0.35038, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 27/35\n",
            "1407/1407 [==============================] - 860s 611ms/step - loss: 0.3206 - binary_accuracy: 0.8642 - val_loss: 0.3551 - val_binary_accuracy: 0.8490\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.35038\n",
            "Epoch 28/35\n",
            "1407/1407 [==============================] - 858s 609ms/step - loss: 0.3186 - binary_accuracy: 0.8657 - val_loss: 0.3464 - val_binary_accuracy: 0.8521\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.35038 to 0.34636, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 29/35\n",
            "1407/1407 [==============================] - 854s 607ms/step - loss: 0.3119 - binary_accuracy: 0.8687 - val_loss: 0.3430 - val_binary_accuracy: 0.8535\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.34636 to 0.34302, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 30/35\n",
            "1407/1407 [==============================] - 852s 605ms/step - loss: 0.3101 - binary_accuracy: 0.8696 - val_loss: 0.3458 - val_binary_accuracy: 0.8529\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.34302\n",
            "Epoch 31/35\n",
            "1407/1407 [==============================] - 852s 605ms/step - loss: 0.3039 - binary_accuracy: 0.8747 - val_loss: 0.3322 - val_binary_accuracy: 0.8625\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.34302 to 0.33218, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 32/35\n",
            "1407/1407 [==============================] - 849s 603ms/step - loss: 0.2945 - binary_accuracy: 0.8810 - val_loss: 0.3295 - val_binary_accuracy: 0.8637\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.33218 to 0.32955, saving model to ./model/incep_res_cd_0219_v1.h5\n",
            "Epoch 33/35\n",
            "1407/1407 [==============================] - 850s 604ms/step - loss: 0.2884 - binary_accuracy: 0.8835 - val_loss: 0.3367 - val_binary_accuracy: 0.8631\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.32955\n",
            "Epoch 34/35\n",
            "1407/1407 [==============================] - 840s 597ms/step - loss: 0.2833 - binary_accuracy: 0.8861 - val_loss: 0.3314 - val_binary_accuracy: 0.8636\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.32955\n",
            "Epoch 35/35\n",
            "1407/1407 [==============================] - 817s 580ms/step - loss: 0.2796 - binary_accuracy: 0.8877 - val_loss: 0.3221 - val_binary_accuracy: 0.8698\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.32955 to 0.32210, saving model to ./model/incep_res_cd_0219_v1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vg0p4ta7dv0"
      },
      "source": [
        "### 훈련데이터를 더 써서 두 번째 훈련\r\n",
        "* 49000 : 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LajHDbGBMWWp",
        "outputId": "785110aa-735b-4652-d291-bd502b9ec2ed"
      },
      "source": [
        "# Data Augumentation\r\n",
        "datagen_ag2 = ImageDataGenerator(rescale=1./255., \r\n",
        "                               rotation_range = 10,\r\n",
        "                               width_shift_range = 0.2,\r\n",
        "                               height_shift_range = 0.2,\r\n",
        "                               horizontal_flip = True,\r\n",
        "                               vertical_flip = True,\r\n",
        "                               validation_split=0.02,\r\n",
        "                               fill_mode = \"nearest\")\r\n",
        "\r\n",
        "# generator_ag\r\n",
        "train_gen_ag2 = datagen_ag2.flow_from_dataframe(dataframe = meta_df,        \r\n",
        "                                        directory = train_dir,       \r\n",
        "                                        x_col='index',               \r\n",
        "                                        y_col=columns,                \r\n",
        "                                        batch_size = 32,               \r\n",
        "                                        seed = 42,\r\n",
        "                                        color_mode = \"rgb\",           \r\n",
        "                                        class_mode='raw',\r\n",
        "                                        target_size=(256, 256),       \r\n",
        "                                        subset='training')\r\n",
        "val_gen_ag2 = datagen_ag2.flow_from_dataframe(dataframe = meta_df,\r\n",
        "                                        directory = train_dir,\r\n",
        "                                        x_col='index',\r\n",
        "                                        y_col=columns,\r\n",
        "                                        batch_size = 32,\r\n",
        "                                        seed = 42,\r\n",
        "                                        color_mode = \"rgb\",\r\n",
        "                                        class_mode='raw',\r\n",
        "                                        target_size=(256, 256),\r\n",
        "                                        subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 49000 validated image filenames.\n",
            "Found 1000 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "allBE5sJ9bQ1",
        "outputId": "e7626ae4-08c8-4e63-ce2b-4c8b43c942e1"
      },
      "source": [
        "# callbacks\r\n",
        "checkpoint2 = keras.callbacks.ModelCheckpoint(f'./model/incep_res_cd_0219_v1_add.h5', \r\n",
        "                                             save_best_only=True, verbose=1)\r\n",
        "early_stop_cb2 = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True)\r\n",
        "\r\n",
        "# fitting\r\n",
        "history = incep_res.fit(train_gen_ag2, epochs = 10, \r\n",
        "                        validation_data = val_gen_ag2, \r\n",
        "                        callbacks = [checkpoint2, early_stop_cb2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1532/1532 [==============================] - 811s 529ms/step - loss: 0.2813 - binary_accuracy: 0.8869 - val_loss: 0.3239 - val_binary_accuracy: 0.8685\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.32388, saving model to ./model/incep_res_cd_0219_v1_add.h5\n",
            "Epoch 2/10\n",
            "1532/1532 [==============================] - 822s 536ms/step - loss: 0.2768 - binary_accuracy: 0.8894 - val_loss: 0.3171 - val_binary_accuracy: 0.8703\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.32388 to 0.31705, saving model to ./model/incep_res_cd_0219_v1_add.h5\n",
            "Epoch 3/10\n",
            "1532/1532 [==============================] - 826s 539ms/step - loss: 0.2731 - binary_accuracy: 0.8909 - val_loss: 0.3142 - val_binary_accuracy: 0.8726\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.31705 to 0.31417, saving model to ./model/incep_res_cd_0219_v1_add.h5\n",
            "Epoch 4/10\n",
            "1532/1532 [==============================] - 826s 539ms/step - loss: 0.2701 - binary_accuracy: 0.8921 - val_loss: 0.3211 - val_binary_accuracy: 0.8670\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.31417\n",
            "Epoch 5/10\n",
            "1532/1532 [==============================] - 821s 536ms/step - loss: 0.2659 - binary_accuracy: 0.8942 - val_loss: 0.3210 - val_binary_accuracy: 0.8722\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.31417\n",
            "Epoch 6/10\n",
            "1532/1532 [==============================] - 852s 556ms/step - loss: 0.2635 - binary_accuracy: 0.8953 - val_loss: 0.3173 - val_binary_accuracy: 0.8710\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.31417\n",
            "Epoch 7/10\n",
            "1532/1532 [==============================] - 853s 557ms/step - loss: 0.2611 - binary_accuracy: 0.8963 - val_loss: 0.3119 - val_binary_accuracy: 0.8736\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.31417 to 0.31187, saving model to ./model/incep_res_cd_0219_v1_add.h5\n",
            "Epoch 8/10\n",
            "1532/1532 [==============================] - 860s 561ms/step - loss: 0.2572 - binary_accuracy: 0.8980 - val_loss: 0.3185 - val_binary_accuracy: 0.8712\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.31187\n",
            "Epoch 9/10\n",
            "1532/1532 [==============================] - 855s 558ms/step - loss: 0.2550 - binary_accuracy: 0.8990 - val_loss: 0.3228 - val_binary_accuracy: 0.8708\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.31187\n",
            "Epoch 10/10\n",
            "  81/1532 [>.............................] - ETA: 13:13 - loss: 0.2486 - binary_accuracy: 0.9018"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-bcee9b69d2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m history = incep_res.fit(train_gen_ag2, epochs = 10, \n\u001b[1;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_gen_ag2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         callbacks = [checkpoint2, early_stop_cb2])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuRNAtQ04_zJ"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBdP27n0bfMo",
        "outputId": "e41774e2-8a0f-40d3-eaf4-8951e1bd5bfb"
      },
      "source": [
        "# image_dataset_from_directory\r\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "    directory = test_dir,\r\n",
        "    label_mode = None,\r\n",
        "    class_names = None,\r\n",
        "    color_mode = \"rgb\",\r\n",
        "    batch_size = 64,\r\n",
        "    image_size = (256, 256),\r\n",
        "    shuffle = False,\r\n",
        "    seed = None,\r\n",
        "    validation_split = None\r\n",
        ")\r\n",
        "\r\n",
        "test_ds = test_ds.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5000 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeWUST-oc-8u"
      },
      "source": [
        "# add rescaling layer\r\n",
        "mod = keras.Sequential([\r\n",
        "                  keras.layers.experimental.preprocessing.Rescaling(1./255.),\r\n",
        "                  incep_res\r\n",
        "])\r\n",
        "\r\n",
        "# prediction\r\n",
        "result = mod.predict(test_ds)\r\n",
        "result2 = result.copy()\r\n",
        "\r\n",
        "# get class label\r\n",
        "result2[result2 <= 0.5] = 0\r\n",
        "result2[result2 > 0.5] = 1\r\n",
        "\r\n",
        "# frame create\r\n",
        "res = pd.DataFrame(result2, columns = columns )\r\n",
        "int_rest = res.astype(int)\r\n",
        "submit = pd.concat([test_df.iloc[:,0],int_rest], axis = 1)\r\n",
        "\r\n",
        "\r\n",
        "submit.to_csv(\"IncepRes_0219_v1.csv\", index = False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc6xCySm-X74",
        "outputId": "58bff0eb-2863-453c-ead4-9c67c2aa38c5"
      },
      "source": [
        "# 모든 데이터 써서 최종 훈련\r\n",
        "\r\n",
        "# Data Augumentation\r\n",
        "datagen_fin = ImageDataGenerator(rescale=1./255.)\r\n",
        "\r\n",
        "# generator_ag\r\n",
        "train_gen_fin = datagen_fin.flow_from_dataframe(dataframe = meta_df,        \r\n",
        "                                        directory = train_dir,       \r\n",
        "                                        x_col='index',               \r\n",
        "                                        y_col=columns,                \r\n",
        "                                        batch_size = 32,               \r\n",
        "                                        seed = 42,\r\n",
        "                                        color_mode = \"rgb\",           \r\n",
        "                                        class_mode='raw',\r\n",
        "                                        target_size=(256, 256),       \r\n",
        "                                        subset='training')\r\n",
        "\r\n",
        "\r\n",
        "# opt\r\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\r\n",
        "BAcc = keras.metrics.BinaryAccuracy()\r\n",
        "\r\n",
        "# model compile\r\n",
        "incep_res.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [BAcc])\r\n",
        "\r\n",
        "\r\n",
        "# callbacks\r\n",
        "checkpoint3 = keras.callbacks.ModelCheckpoint(f'./model/incep_res_cd_0219_v1_fin.h5', verbose=1)\r\n",
        "\r\n",
        "# fitting\r\n",
        "history_fin = incep_res.fit(train_gen_fin, epochs = 2, callbacks = [checkpoint3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 50000 validated image filenames.\n",
            "Epoch 1/2\n",
            "1563/1563 [==============================] - 522s 306ms/step - loss: 0.1179 - binary_accuracy: 0.9588\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 2/2\n",
            "1563/1563 [==============================] - 478s 306ms/step - loss: 0.0701 - binary_accuracy: 0.9764\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wSRDzzOjoZh"
      },
      "source": [
        "incep_res.save('./model/incep_res_cd_0219_v1_fin.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fYQlVCmb1le"
      },
      "source": [
        "mod = keras.Sequential([\r\n",
        "                  keras.layers.experimental.preprocessing.Rescaling(1./255.),\r\n",
        "                  incep_res\r\n",
        "])\r\n",
        "\r\n",
        "# make a prediction on the test set\r\n",
        "yhat = mod.predict(test_ds)\r\n",
        "# round probabilities to class labels\r\n",
        "yhat = yhat.round()\r\n",
        "\r\n",
        "\r\n",
        "# frame create\r\n",
        "res = pd.DataFrame(yhat, columns = columns )\r\n",
        "int_rest = res.astype(int)\r\n",
        "submit = pd.concat([test_df.iloc[:,0],int_rest], axis = 1)\r\n",
        "\r\n",
        "submit.to_csv(\"IncepRes_0219_v2.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}